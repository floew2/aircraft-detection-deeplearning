{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "802eefb6",
   "metadata": {},
   "source": [
    "## Tensorflow Object Detection API:\n",
    "\n",
    "https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/\n",
    "\n",
    "Interesting link to apply TF Object Detection API and pre-calibrated model from model zoo (wind turbines):\n",
    "\n",
    "https://medium.com/analytics-vidhya/tensorflow-object-detection-api-tutorial-wind-turbine-detection-using-google-colab-e8e2e120e54e\n",
    "\n",
    "this is the code: https://github.com/lbborkowski/wind-turbine-detector\n",
    "\n",
    "Maybe also interesting:\n",
    "https://github.com/rhammell/planesnet-detector\n",
    "\n",
    "## YOLO v8: \n",
    "https://www.kaggle.com/code/jeffaudi/aircraft-detection-with-yolov8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae87c94",
   "metadata": {},
   "source": [
    "# Identifying & mapping aircrafts in high-resolution satellite images\n",
    "## Comparing YOLO v8 and TensorFlow Object Detection API\n",
    "\n",
    "TensorFlow provides a powerful library called TensorFlow Object Detection API, which can be utilized to achieve this goal. Here's how this notebook is structured:\n",
    "\n",
    "1. **Prepare Your Dataset**: Obtain a dataset of satellite images that you want to analyze. This dataset should contain the images and annotations for the objects you want to detect in the images (e.g., aircraft, buildings, vehicles, etc.). Annotations should include bounding boxes around the objects and their corresponding class labels.\n",
    "\n",
    "2. **Install TensorFlow and TensorFlow Object Detection API**: Make sure you have TensorFlow installed, and then set up the TensorFlow Object Detection API. You can find installation instructions in the official TensorFlow Object Detection API repository on GitHub.\n",
    "\n",
    "3. **Select a Pre-trained Model**: Choose a pre-trained model from the TensorFlow Model Zoo that suits your object detection needs. These pre-trained models are trained on large datasets like COCO and can serve as a starting point for your analysis.\n",
    "\n",
    "4. **Prepare the Configuration**: Create a configuration file for your selected model. This file defines the model architecture, hyperparameters, and paths to your dataset (training and evaluation data). You'll need to modify the configuration file to match your dataset and requirements.\n",
    "\n",
    "5. **Training**: Train the model using your prepared dataset and the configuration file. Fine-tune the pre-trained model on your satellite images dataset to detect the objects you're interested in.\n",
    "\n",
    "6. **Evaluate the Model**: Once training is complete, evaluate the performance of your model on a separate validation dataset. This step is essential to measure how well your model generalizes to unseen data.\n",
    "\n",
    "7. **Object Detection on New Satellite Images**: After the model has been trained and evaluated, you can use it to perform object detection on new satellite images. The model will predict bounding boxes and class labels for the objects in these images.\n",
    "\n",
    "8. **Visualize the Results**: Display the results of the object detection model on the satellite images to visualize the locations of detected objects.\n",
    "\n",
    "\n",
    "Additionally, you may need to implement post-processing techniques to refine the detected bounding boxes and remove false positives or apply custom object filtering strategies to focus only on specific objects of interest.\n",
    "\n",
    "The TensorFlow Object Detection API documentation provides detailed instructions, tutorials, and examples that can guide you through each step of this process. Make sure to refer to the official documentation to get more detailed and up-to-date information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4321011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required functions from utils.py\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import ast\n",
    "import PIL\n",
    "import utils\n",
    "import shutil\n",
    "import tqdm.notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from config import Config\n",
    "from IPython.display import display, clear_output\n",
    "# !pip install imgaug\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1cae17",
   "metadata": {},
   "source": [
    "## Step 1: Load and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff6f6999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f060fbe498104bc6be1f40070e6b80dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Select an image:', options=('78400c58-1a7c-4342-a1fb-2117cb7cbc8b.jpg', 'f82d64a6-3bfa-4â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the path to your data folder containing images\n",
    "IMAGE_PATH = config.image_folder\n",
    "\n",
    "# Create the dropdown menu\n",
    "dropdown_menu = utils.create_image_dropdown(IMAGE_PATH)\n",
    "\n",
    "# Display the dropdown menu\n",
    "display(dropdown_menu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26bc4196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 103\n",
      "Image Height: 2560\n",
      "Image Width: 2560\n",
      "Number of Channels: 3\n"
     ]
    }
   ],
   "source": [
    "# Get the list of all jpg files in the folder\n",
    "jpg_files = [file for file in os.listdir(IMAGE_PATH) if file.lower().endswith('.jpg')]\n",
    "file_paths = [os.path.join(IMAGE_PATH, file_name) for file_name in jpg_files]\n",
    "\n",
    "# Print the number of JPG files\n",
    "print(\"Number of files:\", len(jpg_files))\n",
    "\n",
    "# Get the details of the first JPG file\n",
    "if jpg_files:\n",
    "    first_image_path = os.path.join(IMAGE_PATH, jpg_files[0])\n",
    "    num_channels, width, height = utils.get_image_details(first_image_path)\n",
    "\n",
    "    # Print the image details\n",
    "    print(\"Image Height:\", height)\n",
    "    print(\"Image Width:\", width)\n",
    "    print(\"Number of Channels:\", num_channels)\n",
    "else:\n",
    "    print(\"No files found in the folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ec319ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>geometry</th>\n",
       "      <th>class</th>\n",
       "      <th>bounds</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4f833867-273e-4d73-8bc3-cb2d9ceb54ef.jpg</td>\n",
       "      <td>[(135, 522), (245, 522), (245, 600), (135, 600...</td>\n",
       "      <td>Aircraft</td>\n",
       "      <td>(135, 522, 245, 600)</td>\n",
       "      <td>110</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4f833867-273e-4d73-8bc3-cb2d9ceb54ef.jpg</td>\n",
       "      <td>[(1025, 284), (1125, 284), (1125, 384), (1025,...</td>\n",
       "      <td>Aircraft</td>\n",
       "      <td>(1025, 284, 1125, 384)</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4f833867-273e-4d73-8bc3-cb2d9ceb54ef.jpg</td>\n",
       "      <td>[(1058, 1503), (1130, 1503), (1130, 1568), (10...</td>\n",
       "      <td>Aircraft</td>\n",
       "      <td>(1058, 1503, 1130, 1568)</td>\n",
       "      <td>72</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4f833867-273e-4d73-8bc3-cb2d9ceb54ef.jpg</td>\n",
       "      <td>[(813, 1518), (885, 1518), (885, 1604), (813, ...</td>\n",
       "      <td>Aircraft</td>\n",
       "      <td>(813, 1518, 885, 1604)</td>\n",
       "      <td>72</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4f833867-273e-4d73-8bc3-cb2d9ceb54ef.jpg</td>\n",
       "      <td>[(594, 938), (657, 938), (657, 1012), (594, 10...</td>\n",
       "      <td>Aircraft</td>\n",
       "      <td>(594, 938, 657, 1012)</td>\n",
       "      <td>63</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                  image_id  \\\n",
       "0   1  4f833867-273e-4d73-8bc3-cb2d9ceb54ef.jpg   \n",
       "1   2  4f833867-273e-4d73-8bc3-cb2d9ceb54ef.jpg   \n",
       "2   3  4f833867-273e-4d73-8bc3-cb2d9ceb54ef.jpg   \n",
       "3   4  4f833867-273e-4d73-8bc3-cb2d9ceb54ef.jpg   \n",
       "4   5  4f833867-273e-4d73-8bc3-cb2d9ceb54ef.jpg   \n",
       "\n",
       "                                            geometry     class  \\\n",
       "0  [(135, 522), (245, 522), (245, 600), (135, 600...  Aircraft   \n",
       "1  [(1025, 284), (1125, 284), (1125, 384), (1025,...  Aircraft   \n",
       "2  [(1058, 1503), (1130, 1503), (1130, 1568), (10...  Aircraft   \n",
       "3  [(813, 1518), (885, 1518), (885, 1604), (813, ...  Aircraft   \n",
       "4  [(594, 938), (657, 938), (657, 1012), (594, 10...  Aircraft   \n",
       "\n",
       "                     bounds  width  height  \n",
       "0      (135, 522, 245, 600)    110      78  \n",
       "1    (1025, 284, 1125, 384)    100     100  \n",
       "2  (1058, 1503, 1130, 1568)     72      65  \n",
       "3    (813, 1518, 885, 1604)     72      86  \n",
       "4     (594, 938, 657, 1012)     63      74  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to the \"annotations.csv\" file\n",
    "FILE_PATH = config.data_folder\n",
    "annotations_file_path = os.path.join(FILE_PATH, \"annotations.csv\")\n",
    "\n",
    "# Function to convert the \"geometry\" column to a list of tuples\n",
    "def convert_geometry(geometry_str):\n",
    "    return ast.literal_eval(geometry_str)\n",
    "\n",
    "# Load the CSV file and apply the necessary conversions\n",
    "annotations_df = pd.read_csv(annotations_file_path, converters={'geometry': convert_geometry, 'class': lambda o: 'Aircraft'})\n",
    "\n",
    "# Create bounds, width, and height columns in annotations_df\n",
    "annotations_df['bounds'], annotations_df['width'], annotations_df['height'] = zip(*annotations_df['geometry'].map(utils.calculate_bounds))\n",
    "\n",
    "# Create a new DataFrame named annotations_df_bounds\n",
    "annotations_df_bounds = annotations_df.copy()\n",
    "\n",
    "# Display the updated DataFrame\n",
    "annotations_df_bounds.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17c3df70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b0b4b4565c454e8fc5a0e9d54109c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create dirs for training and validation data\n",
    "for split in ['train', 'val']:\n",
    "    for directory in [config.tiles_dir[split], config.labels_dir[split]]:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "fold = 1\n",
    "num_fold = 5\n",
    "index = annotations_df_bounds['image_id'].unique()\n",
    "\n",
    "# Calculate the start and end indices for the validation set\n",
    "start_idx = len(index) * fold // num_fold\n",
    "end_idx = len(index) * (fold + 1) // num_fold\n",
    "\n",
    "# Get the validation indexes directly using NumPy slicing\n",
    "val_indexes = index[start_idx:end_idx]\n",
    "\n",
    "# Generate tiles for each image\n",
    "for img_path in tqdm.notebook.tqdm(file_paths):\n",
    "    utils.generate_tiles(img_path=img_path, df=annotations_df_bounds, val_indexes=val_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bac54b",
   "metadata": {},
   "source": [
    "## YOLO v8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7db77d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = \"\"\"\n",
    "# train and val datasets (image directory or *.txt file with image paths)\n",
    "train: data/train/images\n",
    "val: data/val/images\n",
    "\n",
    "# number of classes\n",
    "nc: 1\n",
    "\n",
    "# class names\n",
    "names: ['Aircraft']\n",
    "\"\"\"\n",
    "\n",
    "with open(\"data.yaml\", \"w\") as f:\n",
    "    f.write(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f257cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ultralytics\n",
    "\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "# !yolo checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "329ff3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.147 ðŸš€ Python-3.9.13 torch-2.0.1 CPU (Intel Core(TM) i9-9900K 3.60GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=/Users/fabianlow/Documents/4_CV_Application/5_Portfolios/10_AI_DL_SatelliteImages/2_Airplane_Detection/airplane_detection_git/data.yaml, epochs=10, patience=50, batch=16, imgsz=512, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train7\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11135987 parameters, 11135971 gradients\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train7', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/fabianlow/Documents/4_CV_Application/5_Portfolios/10_AI_D\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/fabianlow/Documents/4_CV_Application/5_Portfolios/10_AI_DL_\u001b[0m\n",
      "Plotting labels to runs/detect/train7/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 512 train, 512 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train7\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10         0G      1.431      1.664      1.325         14        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        756       1459      0.853      0.736       0.81      0.496\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10         0G      1.438      1.033      1.347         29        512:  ^C\n",
      "       2/10         0G      1.438      1.033      1.347         29        512:  \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/fabianlow/opt/anaconda3/bin/yolo\", line 8, in <module>\n",
      "    sys.exit(entrypoint())\n",
      "  File \"/Users/fabianlow/opt/anaconda3/lib/python3.9/site-packages/ultralytics/cfg/__init__.py\", line 423, in entrypoint\n",
      "    getattr(model, mode)(**overrides)  # default args from model\n",
      "  File \"/Users/fabianlow/opt/anaconda3/lib/python3.9/site-packages/ultralytics/engine/model.py\", line 377, in train\n",
      "    self.trainer.train()\n",
      "  File \"/Users/fabianlow/opt/anaconda3/lib/python3.9/site-packages/ultralytics/engine/trainer.py\", line 192, in train\n",
      "    self._do_train(world_size)\n",
      "  File \"/Users/fabianlow/opt/anaconda3/lib/python3.9/site-packages/ultralytics/engine/trainer.py\", line 332, in _do_train\n",
      "    self.loss, self.loss_items = self.model(batch)\n",
      "  File \"/Users/fabianlow/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/fabianlow/opt/anaconda3/lib/python3.9/site-packages/ultralytics/nn/tasks.py\", line 44, in forward\n",
      "    return self.loss(x, *args, **kwargs)\n",
      "  File \"/Users/fabianlow/opt/anaconda3/lib/python3.9/site-packages/ultralytics/nn/tasks.py\", line 213, in loss\n",
      "    preds = self.forward(batch['img']) if preds is None else preds\n",
      "  File \"/Users/fabianlow/opt/anaconda3/lib/python3.9/site-packages/ultralytics/nn/tasks.py\", line 45, in forward\n",
      "    return self.predict(x, *args, **kwargs)\n",
      "  File \"/Users/fabianlow/opt/anaconda3/lib/python3.9/site-packages/ultralytics/nn/tasks.py\", line 62, in predict\n",
      "    return self._predict_once(x, profile, visualize)\n",
      "  File \"/Users/fabianlow/opt/anaconda3/lib/python3.9/site-packages/ultralytics/nn/tasks.py\", line 82, in _predict_once\n",
      "    x = m(x)  # run\n",
      "  File \"/Users/fabianlow/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/fabianlow/opt/anaconda3/lib/python3.9/site-packages/ultralytics/nn/modules/conv.py\", line 38, in forward\n",
      "    return self.act(self.bn(self.conv(x)))\n",
      "  File \"/Users/fabianlow/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/fabianlow/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\", line 463, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/Users/fabianlow/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "HOME = os.getcwd()\n",
    "!yolo task=detect mode=train model=yolov8s.pt data={HOME}/data.yaml epochs=10 imgsz={config.tile_height}\n",
    "\n",
    "#  !python /kaggle/working/yolov5/train.py --cfg yolov5s.yaml --imgsz 512 --batch-size 16 --epochs 10 --data /kaggle/working/dataset.yaml --weights yolov5s.pt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6be64a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls yolov5/runs/train/exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9d8f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/working/runs/detect/train/results.csv\")\n",
    "fig = px.line(df, x='                  epoch', y='       metrics/mAP50(B)', title='mAP50')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1bf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(filename=f'{HOME}/runs/detect/train/val_batch0_pred.jpg', width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a52dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /kaggle/working/yolov5/runs/detect/\n",
    "!python /kaggle/working/yolov5/detect.py --source ../input/airbus-aircrafts-sample-dataset/extra s/ --img-size 2560 --weights /kaggle/working/yolov5/runs/train/exp/weights/best.pt --conf 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62de236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'path/to/validation_results.csv'\n",
    "metrics_to_plot = ['accuracy', 'loss']\n",
    "\n",
    "plot_metrics_from_csv(csv_file, metrics_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60bafd8",
   "metadata": {},
   "source": [
    "## Step 2: Install TensorFlow and TensorFlow Object Detection API"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5fe2d05c",
   "metadata": {},
   "source": [
    "import subprocess\n",
    "\n",
    "# Check if TensorFlow is installed\n",
    "try:\n",
    "    import tensorflow\n",
    "    print(\"TensorFlow is already installed.\")\n",
    "except ImportError:\n",
    "    # Step 1: Install TensorFlow\n",
    "    subprocess.run([\"pip\", \"install\", \"tensorflow\"])\n",
    "    print(\"TensorFlow has been installed.\")\n",
    "\n",
    "# Check if Additional Dependencies for TensorFlow Object Detection API are installed\n",
    "try:\n",
    "    import tf_slim\n",
    "    print(\"Additional dependencies for TensorFlow Object Detection API are already installed.\")\n",
    "except ImportError:\n",
    "    # Step 2: Install Additional Dependencies for TensorFlow Object Detection API\n",
    "    subprocess.run([\"pip\", \"install\", \"tf_slim\", \"lvis\", \"gin-config\"])\n",
    "    print(\"Additional dependencies for TensorFlow Object Detection API have been installed.\")\n",
    "\n",
    "# Check if TensorFlow Models Repository is already cloned\n",
    "import os\n",
    "if os.path.isdir(\"models\"):\n",
    "    print(\"TensorFlow Models Repository is already cloned.\")\n",
    "else:\n",
    "    # Step 3: Clone the TensorFlow Models Repository\n",
    "    subprocess.run([\"git\", \"clone\", \"https://github.com/tensorflow/models.git\"])\n",
    "    print(\"TensorFlow Models Repository has been cloned.\")\n",
    "\n",
    "# Check if Protobuf Compiler is installed\n",
    "try:\n",
    "    subprocess.run([\"protoc\", \"--version\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)\n",
    "    print(\"Protobuf Compiler is already installed.\")\n",
    "except FileNotFoundError:\n",
    "    # Step 4: Install Protobuf Compiler\n",
    "    subprocess.run([\"brew\", \"install\", \"protobuf\"])\n",
    "    print(\"Protobuf Compiler has been installed.\")\n",
    "\n",
    "# Step 5: Compile Protobufs\n",
    "import os\n",
    "os.chdir(\"models/research/\")\n",
    "subprocess.run([\"protoc\", \"object_detection/protos/*.proto\", \"--python_out=.\"])\n",
    "\n",
    "# Step 6: Set PYTHONPATH\n",
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = f\"{os.getcwd()}:{os.getcwd()}/slim:{os.environ.get('PYTHONPATH', '')}\"\n",
    "print(\"PYTHONPATH has been set.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0580575d",
   "metadata": {},
   "source": [
    "## Step 3: Select a Pre-trained Model (e.g., from the TensorFlow Model Zoo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f38677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import config_util\n",
    "from object_detection.builders import model_builder\n",
    "\n",
    "def select_pretrained_model(model_name, num_classes):\n",
    "    # Load the pre-trained model from the TensorFlow Model Zoo\n",
    "    pipeline_config_path = \"path/to/pretrained_models/{}/pipeline.config\".format(model_name)\n",
    "    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n",
    "    model_config = configs['model']\n",
    "    model_config.ssd.num_classes = num_classes\n",
    "\n",
    "    detection_model = model_builder.build(model_config=model_config, is_training=False)\n",
    "    return detection_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae49ec7f",
   "metadata": {},
   "source": [
    "## Step 4: Prepare the Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00425901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_config_file(dataset_path, num_classes, pretrained_model_path):\n",
    "    # Create a configuration file using template or API-specific functions\n",
    "    pipeline_template = \"\"\"\n",
    "    model {\n",
    "      ssd {\n",
    "        num_classes: %d\n",
    "        image_resizer {\n",
    "          fixed_shape_resizer {\n",
    "            height: 2560\n",
    "            width: 2560\n",
    "          }\n",
    "        }\n",
    "        feature_extractor {\n",
    "          type: \"ssd_inception_v2\"\n",
    "        }\n",
    "        box_coder {\n",
    "          faster_rcnn_box_coder {\n",
    "            y_scale: 10.0\n",
    "            x_scale: 10.0\n",
    "            height_scale: 5.0\n",
    "            width_scale: 5.0\n",
    "          }\n",
    "        }\n",
    "        anchor_generator {\n",
    "          ssd_anchor_generator {\n",
    "            num_layers: 6\n",
    "            min_scale: 0.2\n",
    "            max_scale: 0.95\n",
    "            aspect_ratios: 1.0\n",
    "            aspect_ratios: 2.0\n",
    "            aspect_ratios: 0.5\n",
    "            aspect_ratios: 3.0\n",
    "            aspect_ratios: 0.3333\n",
    "          }\n",
    "        }\n",
    "        box_predictor {\n",
    "          convolutional_box_predictor {\n",
    "            min_depth: 0\n",
    "            max_depth: 0\n",
    "            num_layers_before_predictor: 0\n",
    "            use_dropout: false\n",
    "            dropout_keep_probability: 0.8\n",
    "            kernel_size: 1\n",
    "            box_code_size: 4\n",
    "            apply_sigmoid_to_scores: false\n",
    "            conv_hyperparams {\n",
    "              activation: RELU_6,\n",
    "              regularizer {\n",
    "                l2_regularizer {\n",
    "                  weight: 0.00004\n",
    "                }\n",
    "              }\n",
    "              initializer {\n",
    "                truncated_normal_initializer {\n",
    "                  stddev: 0.03\n",
    "                  mean: 0.0\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "        image_resizer {\n",
    "          fixed_shape_resizer {\n",
    "            height: 2560\n",
    "            width: 2560\n",
    "          }\n",
    "        }\n",
    "        box_predictor {\n",
    "          convolutional_box_predictor {\n",
    "            min_depth: 0\n",
    "            max_depth: 0\n",
    "            num_layers_before_predictor: 0\n",
    "            use_dropout: false\n",
    "            dropout_keep_probability: 0.8\n",
    "            kernel_size: 1\n",
    "            box_code_size: 4\n",
    "            apply_sigmoid_to_scores: false\n",
    "            conv_hyperparams {\n",
    "              activation: RELU_6,\n",
    "              regularizer {\n",
    "                l2_regularizer {\n",
    "                  weight: 0.00004\n",
    "                }\n",
    "              }\n",
    "              initializer {\n",
    "                truncated_normal_initializer {\n",
    "                  stddev: 0.03\n",
    "                  mean: 0.0\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    train_config: {\n",
    "      batch_size: 4\n",
    "      num_steps: 200000\n",
    "      fine_tune_checkpoint: \"%s\"\n",
    "      fine_tune_checkpoint_type: \"detection\"\n",
    "      use_bfloat16: false\n",
    "      fine_tune_checkpoint_version: V2\n",
    "      fine_tune_checkpoint_type: \"detection\"\n",
    "    }\n",
    "    train_input_reader: {\n",
    "      label_map_path: \"path/to/label_map.pbtxt\"\n",
    "      tf_record_input_reader {\n",
    "        input_path: \"path/to/training_tfrecord.record\"\n",
    "      }\n",
    "    }\n",
    "    eval_config: {\n",
    "      metrics_set: \"coco_detection_metrics\"\n",
    "    }\n",
    "    eval_input_reader: {\n",
    "      label_map_path: \"path/to/label_map.pbtxt\"\n",
    "      shuffle: false\n",
    "      num_epochs: 1\n",
    "      tf_record_input_reader {\n",
    "        input_path: \"path/to/testing_tfrecord.record\"\n",
    "      }\n",
    "    }\n",
    "    \"\"\" % (num_classes, pretrained_model_path)\n",
    "\n",
    "    config_file_path = \"path/to/your/config_file.config\"\n",
    "    with open(config_file_path, 'w') as config_file:\n",
    "        config_file.write(pipeline_template)\n",
    "    \n",
    "    return config_file_path\n",
    "\n",
    "# Assuming you have already downloaded a pre-trained model checkpoint from the Model Zoo\n",
    "pretrained_model_path = \"path/to/pretrained_model.ckpt\"\n",
    "num_classes = 1  # Assuming you want to detect one class (e.g., aircraft)\n",
    "config_file_path = create_config_file(training_dataset_path, num_classes, pretrained_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ae7919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03ef31ad",
   "metadata": {},
   "source": [
    "## Step 5: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead88f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection import model_lib_v2\n",
    "\n",
    "def train_model(config_file_path):\n",
    "    # Start the training process\n",
    "    model_lib_v2.train_loop(config_override=config_file_path)\n",
    "\n",
    "# Train the model\n",
    "train_model(config_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276962f9",
   "metadata": {},
   "source": [
    "## Step 6: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c440430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection import model_lib_v2\n",
    "\n",
    "def evaluate_model(config_file_path):\n",
    "    # Start the evaluation process\n",
    "    model_lib_v2.eval_continuously(config_override=config_file_path)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(config_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7927709c",
   "metadata": {},
   "source": [
    "## Step 7: Object Detection on New Satellite Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4622327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects(image_path, model):\n",
    "    # Load the image from the provided path\n",
    "    image = load_image(image_path)\n",
    "    \n",
    "    # Perform object detection on the image using the model\n",
    "    # You may need to adapt this code based on the API of the selected model\n",
    "    detections = model.detect_objects(image)\n",
    "    \n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a22ce6",
   "metadata": {},
   "source": [
    "## Step 8: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d856d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualize_results(image, detections, annotations=None):\n",
    "    # Visualize the image with bounding boxes and class labels based on the detections\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for detection in detections:\n",
    "        class_id, score, bbox = detection\n",
    "        x, y, width, height = bbox\n",
    "        rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x, y, f'Class: {class_id}, Score: {score:.2f}', fontsize=12, color='r')\n",
    "\n",
    "    if annotations:\n",
    "        for annotation in annotations:\n",
    "            x_coords, y_coords = zip(*annotation)\n",
    "            rect = patches.Polygon(list(zip(x_coords, y_coords)), linewidth=2, edgecolor='g', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
